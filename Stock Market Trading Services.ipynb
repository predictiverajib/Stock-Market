{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Bank](http://www.abm.co.uk/banking-finance-services/wp-content/uploads/sites/92/2016/04/Banking_Icons_Frame4c.jpg)"},{"metadata":{},"cell_type":"markdown","source":"A financial institution is planning to roll out a stock market trading faciliation service for their existing\naccount holders. This service costs significant amount of money for the bank in terms of infra, licensing\nand people cost. To make the serive offering profitable, they charge a percentage base comission on every\ntrade transaction. However this is not a unique service offered by them, many of their other competitors are\noffering the same service and at lesser commission some times. To retain or attract people who trade heavily\non stock market and in turn generate a good commission for institution, they are planning to offer discounts\nas they roll out the service to entire customer base.\n\nProblem is , that this discount, hampers profits coming from the customers who do not trade in large\nquantities . To tackle this issue , company wants to offer discounts selectively. To be able to do so, they need\nto know which of their customers are going to be heavy traders or money makers for them.\nTo be able to do this, they decided to do a beta run of their service to a small chunk of their customer base\n[approx 10000 people]. For these customers they have manually divided them into two revenue categories 1\nand 2. Revenue one category is the one which are moeny makers for the bank, revenue category 2 are the\nones which need to be kept out of discount offers."},{"metadata":{},"cell_type":"markdown","source":"We need to use this study’s data to build a prediction model which should be able to identify if a customer is\npotentially eligible for discounts [falls In revnue grid category 1]. Lets get the data and begin."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nrg_train=read.csv(\"../input/rg_train.csv\",stringsAsFactors=F)\nrg_test=read.csv(\"../input/rg_test.csv\",stringsAsFactors=F)\n\n## Let's take a sneak peak of our train & test data \nhead(rg_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable names are self explanatory as to what they represent. Our train data contains 32 variables like age_band,status,Occupation,home status etc. which will be involved in modelling process. Similarily for our test data given below there are 31 variables excluding that response variable (Revenue Grid) which we will predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"head(rg_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(dplyr)\nglimpse(rg_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glimpse(rg_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Preparation:- \n\nWe’ll combine our two datasets so that we do not need to prepare data separately for them. And we’ll also\navoid problem of dealing with different columns in different datasets.\nHowever before combining them, we’ll need to add response column to test because number of columns need\nto be same for two datasets to stack vertically."},{"metadata":{"trusted":true},"cell_type":"code","source":"rg_test$Revenue.Grid=NA\nrg_train$data='train'\nrg_test$data='test'\nrg=rbind(rg_train,rg_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets start with looking at our first predictor variable in the data which is\n“children”."},{"metadata":{"trusted":true},"cell_type":"code","source":"table(rg$children)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From glimpse function above we saw that the variable 'children' has come as character inspite of being numeric. We can easily convert this, to numeric data without any concern."},{"metadata":{"trusted":true},"cell_type":"code","source":"rg = rg %>%\nmutate(children=ifelse(children==\"Zero\",0,substr(children,1,1)),\nchildren=as.numeric(children))\n\nTo check whether it has been converted to numeric or not we will run glimpse again\nglimpse(rg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at age band variable , we can possibly convert this to numeric by taking average of age ranges.\nLets look at the frequency table any way to find if there are any non-numeric fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"table(rg$age_band)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rg=rg %>%\nmutate(a1=as.numeric(substr(age_band,1,2)),\na2=as.numeric(substr(age_band,4,5)),\nage=ifelse(substr(age_band,1,2)==\"71\",71,ifelse(age_band==\"Unknown\",NA,0.5*(a1+a2)))\n) %>%\nselect(-a1,-a2,-age_band)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glimpse(rg)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we’ll be looking at various categorical variables & create dummies for them. Instead of converting one by one, alternatively we can write a function which should ignore categories with low count.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"CreateDummies=function(data,var,freq_cutoff=0){\nt=table(data[,var])\nt=t[t>freq_cutoff]\nt=sort(t)\ncategories=names(t)[-1]\nfor( cat in categories){\nname=paste(var,cat,sep=\"_\")\nname=gsub(\" \",\"\",name)\nname=gsub(\"-\",\"_\",name)\nname=gsub(\"\\\\?\",\"Q\",name)\nname=gsub(\"<\",\"LT_\",name)\nname=gsub(\"\\\\+\",\"\",name)\nname=gsub(\"\\\\/\",\"_\",name)\nname=gsub(\">\",\"GT_\",name)\nname=gsub(\"=\",\"EQ_\",name)\nname=gsub(\",\",\"\",name)\ndata[,name]=as.numeric(data[,var]==cat)\n}\ndata[,var]=NULL\nreturn(data)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let me explain the function CreateDummies which we created above.\n\nt=table(data[,var]) this bit creates a frequency table for the given categorical column. t here is now\nsimply a table which contains names as categories of the categorical variable and their frequency in the data.\nt=t[t>freq_cutoff] this line of code removes those categories from the table which have frequencies below\nthe frequency cutoff. ( this is a subjective choice)\n‘t=sort(t)’ this line simple sorts the remaining table in ascending order\ncategories=names(t)[-1] since we sorted the table in ascending manner in the previous line, first category\nhere has least count. In this line of code we are taking out the category names except the first one ( which\nhas least count), thus making n-1 dummies from the remaining categories.\nname=paste(var,cat,sep=\"_\") all the dummy vars that we intend to make, need to have some name. this\nline of code creates that name by concatenating variable name with category name with an _.\nname=gsub(\" \",\"\",name) subsequent lines like these using gsub are essentially cleaning up the name thats\nall. Since we dont have any control over what the categories can be, we are removing special characters and\nspaces in the code in an automated fashion.\ndata[,name]=as.numeric(data[,var]==cat) once we have a cleaned up name, this line creates the dummy\nvar for that particular category.\ndata[,var]=NULL once we are done creating dummies for the variable using for loop. Variable is removed\nfrom the data in this line."},{"metadata":{},"cell_type":"markdown","source":"Following a bit of code which can be used to extract names of all categorical variables in the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"names(rg)[sapply(rg,function(x) is.character(x))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to ignore column 'data' for obvious reasons and make dummies for rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols=c(\"status\",\"occupation\",\"occupation_partner\",\"home_status\",\"family_income\",\"self_employed\",\n\"self_employed_partner\",\"TVarea\",\"gender\",\"region\")\nfor(cat in cat_cols){\nrg=CreateDummies(rg,cat,100)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are dropping variables post_code, post_area. They take too many distinct values for these variables to\nbe useful in modeling process."},{"metadata":{"trusted":true},"cell_type":"code","source":"rg=rg %>%\nselect(-post_code,-post_area)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to convert our Response variable to 1/0"},{"metadata":{"trusted":true},"cell_type":"code","source":"rg$Revenue.Grid=as.numeric(rg$Revenue.Grid==1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we take care of missing values if any in the data. We will run a for loop as below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for(col in names(rg)){\nif(sum(is.na(rg[,col]))>0 & !(col %in% c(\"data\",\"Revenue.Grid\"))){\nrg[is.na(rg[,col]),col]=mean(rg[rg$data=='train',col],na.rm=T)\n}\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets separate our datasets to begin modeling process"},{"metadata":{"trusted":true},"cell_type":"code","source":"rg_train=rg %>% filter(data=='train') %>% select(-data)\nrg_test=rg %>% filter(data=='test') %>% select (-data,-Revenue.Grid)\n\n## Now our train data contains 77 variables instead of 32. \nglimpse(rg_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Similarily for test data, variables got increased from 31 to 76 because of inclusion of dummy variables. \n\nglimpse(rg_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we want to look at tentative performance measure, we’ll break our data into two parts. "},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(2)\ns=sample(1:nrow(rg_train),0.8*nrow(rg_train))\nrg_train1=rg_train[s,]\nrg_train2=rg_train[-s,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First thing that we’ll be looking to eliminate is severe cases of multi-collinearity. . To examine VIF, we can run a linear regression. We are not concerned with the output of this linear regression model, we are only interested in VIF values of the predictor."},{"metadata":{"trusted":true},"cell_type":"code","source":"library(car)\nfor_vif=lm(Revenue.Grid~.-REF_NO,data=rg_train1)\nsort(vif(for_vif),decreasing = T)[1:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are few cases of insanely high VIF values , lets eliminate those variables one by one. Code\ngiven below is result of multiple iterations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for_vif=lm(Revenue.Grid~.-REF_NO-Investment.in.Commudity\n-Investment.in.Derivative-Investment.in.Equity\n-region_SouthEast-TVarea_Central-occupation_Professional\n-family_income_GT_EQ_35000-region_Scotland\n-Portfolio.Balance,\ndata=rg_train1)\nsort(vif(for_vif),decreasing = T)[1:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All VIF values now are less than 10. This is good enough for logistic regression , Lets move to build our\nclassification model now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_fit=glm(Revenue.Grid~.-REF_NO-Investment.in.Commudity\n-Investment.in.Derivative-Investment.in.Equity\n-region_SouthEast-TVarea_Central-occupation_Professional\n-family_income_GT_EQ_35000-region_Scotland-Portfolio.Balance,data=rg_train1,family = \"binomial\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_fit=step(log_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(log_fit)\n\n## If we look at summary(log_fit), we’ll find there are still some variable with high p-values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will run our logistic regression model with variables selected by step\nfunction and now drop variabe based on p-values on our own from the remaining bunch."},{"metadata":{"trusted":true},"cell_type":"code","source":"formula(log_fit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use this to now run our model and drop variables based on p-values too. Code given below is result\nof multiple iteration. We have considered 0.1 as p-value cutoff, you can make it lower and drop more variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_fit=glm(Revenue.Grid ~ Average.Credit.Card.Transaction + Balance.Transfer +\nTerm.Deposit + Life.Insurance + Medical.Insurance + Average.A.C.Balance +\nPersonal.Loan + Investment.in.Mutual.Fund + Investment.Tax.Saving.Bond +\nHome.Loan + Online.Purchase.Amount +\nfamily_income_LT_30000GT_EQ_27500 +\nself_employed_partner_No + TVarea_ScottishTV + TVarea_Meridian ,\ndata=rg_train,family='binomial')\nsummary(log_fit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see performance of score model on validation data that we kept aside."},{"metadata":{"trusted":true},"cell_type":"code","source":"## We will be using library pROC\nlibrary(pROC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val.score=predict(log_fit,newdata = rg_train2,type='response')\nauc_score=auc(roc(rg_train2$Revenue.Grid,val.score))\nauc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The area under the curve is 0.96. So the tentative score performance of logistic regression is going to be around 0.96. Lets visualise how is our eventual binary response is behaving w.r.t. score that we obtained. "},{"metadata":{"trusted":true},"cell_type":"code","source":"library(ggplot2)\nmydata=data.frame(Revenue.Grid=rg_train2$Revenue.Grid,val.score=val.score)\nggplot(mydata,aes(y=Revenue.Grid,x=val.score,color=factor(Revenue.Grid)))+\ngeom_point()+geom_jitter()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that response 0 is bunched around low scores and response 1 is bunched around high scores,\nHowever there is overlap as well across score values."},{"metadata":{},"cell_type":"markdown","source":"We know the tentative performance of logistic regression model in terms of auc score. Next we’ll build the\nmodel on entire training data following the similar steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"for_vif=lm(Revenue.Grid~.-REF_NO-Investment.in.Commudity\n-Investment.in.Derivative-Investment.in.Equity\n-region_SouthEast-TVarea_Central-occupation_Professional\n-family_income_GT_EQ_35000-region_Scotland-Portfolio.Balance\n,data=rg_train)\nsort(vif(for_vif),decreasing = T)[1:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log.fit.final=glm(Revenue.Grid~.-REF_NO-Investment.in.Commudity\n-Investment.in.Derivative-Investment.in.Equity\n-region_SouthEast-TVarea_Central-occupation_Professional\n-family_income_GT_EQ_35000-region_Scotland-Portfolio.Balance,\ndata=rg_train,family='binomial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log.fit.final=step(log.fit.final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log.fit.final=glm(Revenue.Grid ~ Average.Credit.Card.Transaction + Balance.Transfer +\nTerm.Deposit + Life.Insurance + Medical.Insurance + Average.A.C.Balance +\nPersonal.Loan + Investment.in.Mutual.Fund + Investment.Tax.Saving.Bond +\nHome.Loan + Online.Purchase.Amount + status_Partner +\noccupation_partner_Retired+\nself_employed_partner_No + TVarea_ScottishTV + TVarea_Meridian +\ngender_Female,\ndata=rg_train,family='binomial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(log.fit.final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now if we need to submit simple probability score we can make prediction on test data and submit that."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.prob.score= predict(log_fit,newdata = rg_test,type='response')\n\ntest.prob.score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can save this csv file in a location. "},{"metadata":{"trusted":true},"cell_type":"code","source":"write.csv(test.prob.score,\"proper_submission_file_name.csv\",row.names = F)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}